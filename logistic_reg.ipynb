{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape (100, 100)\n",
      "X example [-0.95525507 -2.5393372   1.04320624  2.25727852 -1.32246001 -0.65136624\n",
      " -1.22814486  0.28531415  0.42977981 -1.46231784 -0.97223222 -0.50967561\n",
      "  0.28734442 -0.19864146 -0.45020693 -1.35166643 -1.74771416  0.2511429\n",
      "  0.90821607  0.09671154  0.05002272  1.70745186  0.09940079 -0.20171883\n",
      "  0.81018488  1.12933264  1.21835718  0.86590791  0.43400353 -0.34388696\n",
      "  0.55569143 -1.10387368 -0.80699393  0.41525958  0.4962095  -1.63839207\n",
      "  0.65135464 -0.16946154 -0.21845057 -1.17545363 -0.36455252 -0.1891814\n",
      "  0.17032391  0.72153018 -0.22610354  1.26059283  0.32955441  1.25376854\n",
      "  0.90310513  1.05572704 -0.35800126 -0.29762561  2.05085754 -0.3766795\n",
      " -0.28281631 -0.89676154  0.27406499 -0.82695734 -1.69346651  2.11165371\n",
      " -0.01710709  0.95013389  2.01530051  1.00597632  0.97939386  0.00726233\n",
      " -0.27416538  1.36266566 -0.05868107  0.18660559  1.07128523  1.57668939\n",
      "  0.71624157  0.65465647  0.80831849 -1.26807624  0.59680123 -1.29008354\n",
      "  1.16917763  0.41091497 -0.279837   -0.38646272  1.53829513  2.99347732\n",
      " -1.5623799  -0.82706994  1.95193482 -0.39863182  1.03393114 -1.05993551\n",
      "  0.83161392 -0.90084521  1.46067629  1.26267362 -0.03665665  1.31095821\n",
      " -0.76971067  1.63266987  0.72825482 -0.53120195]\n",
      "y shape (100,)\n",
      "y example 1\n"
     ]
    }
   ],
   "source": [
    "n_features = 100\n",
    "n_observations = 100\n",
    "\n",
    "X = np.random.randn(n_observations, n_features)\n",
    "y = np.random.randint(0, 3, n_observations)\n",
    "\n",
    "print(\"X shape\", X.shape)\n",
    "print(\"X example\", X[0])\n",
    "print(\"y shape\", y.shape)\n",
    "print(\"y example\", y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression()\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/log_reg_model.joblib']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import dump\n",
    "\n",
    "dump(clf, \"models/log_reg_model.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference time: 2.14s\n",
      "File size: 3.205 KB\n",
      "Test rows: 10.0 million\n"
     ]
    }
   ],
   "source": [
    "# Check file size\n",
    "from utils import execution_time\n",
    "\n",
    "test_rows = 10000000\n",
    "\n",
    "X_test = np.random.rand(test_rows, n_features)\n",
    "\n",
    "inference_time = execution_time(clf.predict(X_test))\n",
    "weights_size = Path(\"models/log_reg_model.joblib\").stat().st_size\n",
    "\n",
    "print(f\"Inference time: {inference_time:.2f}s\")\n",
    "print(f\"File size: {weights_size / 1000} KB\")\n",
    "print(f\"Test rows: {(test_rows / 1000000)} million\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export model as ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skl2onnx import convert_sklearn\n",
    "from skl2onnx.common.data_types import FloatTensorType\n",
    "\n",
    "initial_type = [('float_input', FloatTensorType([None, n_features]))]\n",
    "onx = convert_sklearn(clf, initial_types=initial_type)\n",
    "with open(\"models/log_reg_model.onnx\", \"wb\") as f:\n",
    "    f.write(onx.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as rt\n",
    "\n",
    "sess = rt.InferenceSession(\"models/log_reg_model.onnx\")\n",
    "input_name = sess.get_inputs()[0].name\n",
    "label_name = sess.get_outputs()[0].name\n",
    "\n",
    "X_test_onnx = X_test.astype(np.float32)\n",
    "\n",
    "pred_onx = sess.run([label_name], {input_name: X_test_onnx})[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference time: 2.27s\n",
      "File size: 2.137 KB\n",
      "Test rows: 10.0 million\n"
     ]
    }
   ],
   "source": [
    "test_rows = 10000000\n",
    "\n",
    "X_test = np.random.rand(test_rows, 4)\n",
    "\n",
    "inference_time = execution_time(lambda: sess.run([label_name], {input_name: X_test_onnx}))\n",
    "weights_size = Path(\"models/log_reg_model.onnx\").stat().st_size\n",
    "\n",
    "print(f\"Inference time: {inference_time:.2f}s\")\n",
    "print(f\"File size: {weights_size / 1000} KB\")\n",
    "print(f\"Test rows: {(test_rows / 1000000)} million\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "92f6c8b6bdf7ebb20c93336518ddb8fd8dee2042dce8e5033c2591f230ecc507"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('onnx')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
